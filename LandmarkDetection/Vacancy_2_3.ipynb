{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e84b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import matplotlib.image as mpimg\n",
    "from collections import OrderedDict\n",
    "from skimage import io, transform\n",
    "from math import *\n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b3261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transforms():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def rotate(self, image, landmarks, angle):\n",
    "        angle = random.uniform(-angle, +angle)\n",
    "\n",
    "        transformation_matrix = torch.tensor([\n",
    "            [+cos(radians(angle)), -sin(radians(angle))], \n",
    "            [+sin(radians(angle)), +cos(radians(angle))]\n",
    "        ])\n",
    "\n",
    "        image = imutils.rotate(np.array(image), angle)\n",
    "\n",
    "        landmarks = landmarks - 112\n",
    "        new_landmarks = np.matmul(landmarks, transformation_matrix)\n",
    "        new_landmarks = new_landmarks + 112\n",
    "        return Image.fromarray(image), new_landmarks\n",
    "\n",
    "    def resize(self, image, landmarks, img_size):\n",
    "        ###\n",
    "        landmarks = landmarks / torch.tensor([np.array(image).shape[0]/224, np.array(image).shape[0]/224])\n",
    "        ###\n",
    "        image = TF.resize(image, img_size)\n",
    "        return image, landmarks\n",
    "\n",
    "    def color_jitter(self, image, landmarks):\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3, \n",
    "                                              contrast=0.3,\n",
    "                                              saturation=0.3, \n",
    "                                              hue=0.1)\n",
    "        image = color_jitter(image)\n",
    "        return image, landmarks\n",
    "\n",
    "    def crop_face(self, image, landmarks, crops):\n",
    "        left = int(crops['left'])\n",
    "        top = int(crops['top'])\n",
    "        width = int(crops['width'])\n",
    "        height = int(crops['height'])\n",
    "        \n",
    "        if(width > height) :\n",
    "            left = left - width*0.25\n",
    "            top = top + height*0.5 - width*0.5 - width*0.25\n",
    "            width = width*1.5\n",
    "            height = width\n",
    "        else :\n",
    "            top = top - height*0.25\n",
    "            left = left + width*0.5 - height*0.5 - height*0.25\n",
    "            height = height*1.5\n",
    "            width = height\n",
    "\n",
    "        image = TF.crop(image, top, left, height, width)\n",
    "        \n",
    "        img_shape = np.array(image).shape\n",
    "        landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]])\n",
    "        #landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]])\n",
    "        return image, landmarks\n",
    "\n",
    "    def __call__(self, image, landmarks, crops):\n",
    "        image = Image.fromarray(image)\n",
    "        image, landmarks = self.crop_face(image, landmarks, crops)\n",
    "        image, landmarks = self.resize(image, landmarks, (224, 224))\n",
    "        image, landmarks = self.color_jitter(image, landmarks)\n",
    "        image, landmarks = self.rotate(image, landmarks, angle=10)\n",
    "\n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        \n",
    "        return image, landmarks\n",
    "\n",
    "class Transforms_Flip():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def flip(self, image, landmarks):\n",
    "        image = cv2.flip(np.array(image), 1)\n",
    "        landmarks[:, 0] = image.shape[1] - landmarks[:, 0]\n",
    "        return Image.fromarray(image), landmarks\n",
    "    \n",
    "    def rotate(self, image, landmarks, angle):\n",
    "        angle = random.uniform(-angle, +angle)\n",
    "\n",
    "        transformation_matrix = torch.tensor([\n",
    "            [+cos(radians(angle)), -sin(radians(angle))], \n",
    "            [+sin(radians(angle)), +cos(radians(angle))]\n",
    "        ])\n",
    "\n",
    "        image = imutils.rotate(np.array(image), angle)\n",
    "\n",
    "        landmarks = landmarks - 112\n",
    "        new_landmarks = np.matmul(landmarks, transformation_matrix)\n",
    "        new_landmarks = new_landmarks + 112\n",
    "        return Image.fromarray(image), new_landmarks\n",
    "\n",
    "    def resize(self, image, landmarks, img_size):\n",
    "        ###\n",
    "        landmarks = landmarks / torch.tensor([np.array(image).shape[0]/224, np.array(image).shape[0]/224])\n",
    "        ###\n",
    "        image = TF.resize(image, img_size)\n",
    "        return image, landmarks\n",
    "\n",
    "    def color_jitter(self, image, landmarks):\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3, \n",
    "                                              contrast=0.3,\n",
    "                                              saturation=0.3, \n",
    "                                              hue=0.1)\n",
    "        image = color_jitter(image)\n",
    "        return image, landmarks\n",
    "\n",
    "    def crop_face(self, image, landmarks, crops):\n",
    "        left = int(crops['left'])\n",
    "        top = int(crops['top'])\n",
    "        width = int(crops['width'])\n",
    "        height = int(crops['height'])\n",
    "        \n",
    "        if(width > height) :\n",
    "            left = left - width*0.25\n",
    "            top = top + height*0.5 - width*0.5 - width*0.25\n",
    "            width = width*1.5\n",
    "            height = width\n",
    "        else :\n",
    "            top = top - height*0.25\n",
    "            left = left + width*0.5 - height*0.5 - height*0.25\n",
    "            height = height*1.5\n",
    "            width = height\n",
    "\n",
    "        image = TF.crop(image, top, left, height, width)\n",
    "        \n",
    "        img_shape = np.array(image).shape\n",
    "        landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]])\n",
    "        #landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]])\n",
    "        return image, landmarks\n",
    "\n",
    "    def __call__(self, image, landmarks, crops):\n",
    "        image = Image.fromarray(image)\n",
    "        image, landmarks = self.crop_face(image, landmarks, crops)\n",
    "        image, landmarks = self.resize(image, landmarks, (224, 224))\n",
    "        image, landmarks = self.color_jitter(image, landmarks)\n",
    "        image, landmarks = self.rotate(image, landmarks, angle=10)\n",
    "        image, landmarks = self.flip(image, landmarks)\n",
    "            \n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        \n",
    "        return image, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df60fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        tree = ET.parse('./300W/labels_300W_new_1.xml')\n",
    "        root = tree.getroot()\n",
    "\n",
    "        self.image_filenames = []\n",
    "        self.landmarks = []\n",
    "        self.crops = []\n",
    "        self.transform = transform\n",
    "        self.root_dir= './300W'\n",
    "        \n",
    "        \n",
    "        index_k = 0\n",
    "        for filename in root[0]:\n",
    "            landmark = []\n",
    "            for num in range(68):\n",
    "                x_coordinate = int(filename[0][num].attrib['x'])\n",
    "                y_coordinate = int(filename[0][num].attrib['y'])\n",
    "                landmark.append([x_coordinate, y_coordinate])\n",
    "            \n",
    "            y_l = landmark[37][1], landmark[41][1], landmark[38][1], landmark[40][1]  # left eye\n",
    "            y_r = landmark[43][1], landmark[47][1], landmark[44][1], landmark[46][1]  # right eye\n",
    "            y_n = landmark[33][1], landmark[51][1] # nose tip to top of mouth\n",
    "            avg = 0\n",
    "            for j in range(2) :\n",
    "                avg = avg - y_l[2*j] + y_l[2*j+1] - y_r[2*j] + y_r[2*j+1]\n",
    "            avg = avg/4\n",
    "            nom = (-y_n[0]+y_n[1])\n",
    "            if nom == 0 :\n",
    "                nom = 1\n",
    "            val = avg/nom\n",
    "            \n",
    "            if val> 0.2:\n",
    "                index_k += 1\n",
    "                self.image_filenames.append(os.path.join(self.root_dir, filename.attrib['file']))\n",
    "                self.crops.append(filename[0].attrib)\n",
    "                self.landmarks.append(landmark)                \n",
    "            elif val <= 0.2:\n",
    "                for i in range(28) :\n",
    "                    self.image_filenames.append(os.path.join(self.root_dir, filename.attrib['file']))\n",
    "                    self.crops.append(filename[0].attrib)\n",
    "                    self.landmarks.append(landmark)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.image_filenames[index], 0)\n",
    "        landmarks = self.landmarks[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image, landmarks = self.transform(image, landmarks, self.crops[index])\n",
    "\n",
    "        #landmarks = landmarks - 0.5\n",
    "\n",
    "        return image, landmarks\n",
    "    \n",
    "dataset = FaceLandmarksDataset(Transforms())\n",
    "#dataset_flip = FaceLandmarksDataset(Transforms_Flip())\n",
    "#dataset = dataset + dataset_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ad632e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8398\n",
      "tensor([53.6940, 99.9176])\n",
      "tensor([54.5710, 87.2727])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "idx = int(len(dataset)/2)\n",
    "print(dataset[0][1][0])\n",
    "print(dataset[idx][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1efce549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of Train set is 7559\n",
      "The length of Valid set is 839\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into validation and test sets\n",
    "len_valid_set = int(0.1*len(dataset))\n",
    "len_train_set = len(dataset) - len_valid_set\n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_set))\n",
    "\n",
    "train_dataset , valid_dataset,  = torch.utils.data.random_split(dataset , [len_train_set, len_valid_set])\n",
    "\n",
    "# shuffle and batch the datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=2, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37396ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, landmarks = next(iter(train_loader))\n",
    "plt.figure(figsize=(600/100, 600/100), dpi=100)\n",
    "plt.scatter(landmarks[0,:,0], landmarks[0,:,1], c = 'g', s = 10)\n",
    "plt.imshow(images[0].cpu().numpy().transpose(1,2,0).squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c4b5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,num_classes=136):\n",
    "        super().__init__()\n",
    "        self.model_name='resnet34'\n",
    "        self.model=models.resnet34(pretrained=False)\n",
    "        #self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.conv1=nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=2, bias=False)\n",
    "        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81205d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def print_overwrite(step, total_step, loss, operation):\n",
    "    sys.stdout.write('\\r')\n",
    "    if operation == 'train':\n",
    "        sys.stdout.write(\"Train Steps: %d/%d  Loss: %.4f \" % (step, total_step, loss))   \n",
    "    else:\n",
    "        sys.stdout.write(\"Valid Steps: %d/%d  Loss: %.4f \" % (step, total_step, loss))\n",
    "        \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb1807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Steps: 420/420  Loss: 58.1597  \n",
      "--------------------------------------------------\n",
      "Epoch: 1  Train Loss: 90.6656  Valid Loss: 58.1597\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 58.1597 at epoch 1/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 379.3362374305725 s\n",
      "Valid Steps: 420/420  Loss: 15.0348 \n",
      "--------------------------------------------------\n",
      "Epoch: 2  Train Loss: 33.0378  Valid Loss: 15.0348\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 15.0348 at epoch 2/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 377.6037690639496 s\n",
      "Valid Steps: 420/420  Loss: 9.1413  \n",
      "--------------------------------------------------\n",
      "Epoch: 3  Train Loss: 9.7401  Valid Loss: 9.1413\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 9.1413 at epoch 3/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 379.67619729042053 s\n",
      "Valid Steps: 420/420  Loss: 9.4131  \n",
      "--------------------------------------------------\n",
      "Epoch: 4  Train Loss: 8.6471  Valid Loss: 9.4131\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 9.2922 \n",
      "--------------------------------------------------\n",
      "Epoch: 5  Train Loss: 8.5834  Valid Loss: 9.2922\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 8.6617 \n",
      "--------------------------------------------------\n",
      "Epoch: 6  Train Loss: 8.4362  Valid Loss: 8.6617\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 8.6617 at epoch 6/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 373.9467148780823 s\n",
      "Valid Steps: 420/420  Loss: 8.5928 \n",
      "--------------------------------------------------\n",
      "Epoch: 7  Train Loss: 8.1416  Valid Loss: 8.5928\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 8.5928 at epoch 7/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 368.3508949279785 s\n",
      "Valid Steps: 420/420  Loss: 7.0011 \n",
      "--------------------------------------------------\n",
      "Epoch: 8  Train Loss: 7.4378  Valid Loss: 7.0011\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 7.0011 at epoch 8/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 364.72486782073975 s\n",
      "Valid Steps: 420/420  Loss: 6.4590 \n",
      "--------------------------------------------------\n",
      "Epoch: 9  Train Loss: 6.1656  Valid Loss: 6.4590\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 6.4590 at epoch 9/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 383.4526858329773 s\n",
      "Valid Steps: 420/420  Loss: 5.0250 \n",
      "--------------------------------------------------\n",
      "Epoch: 10  Train Loss: 5.0633  Valid Loss: 5.0250\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 5.0250 at epoch 10/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 386.70894026756287 s\n",
      "Valid Steps: 420/420  Loss: 4.2509 \n",
      "--------------------------------------------------\n",
      "Epoch: 11  Train Loss: 4.2333  Valid Loss: 4.2509\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 4.2509 at epoch 11/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 386.6614987850189 s\n",
      "Valid Steps: 420/420  Loss: 3.7650 \n",
      "--------------------------------------------------\n",
      "Epoch: 12  Train Loss: 3.9012  Valid Loss: 3.7650\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 3.7650 at epoch 12/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 464.19265484809875 s\n",
      "Valid Steps: 420/420  Loss: 3.9236 \n",
      "--------------------------------------------------\n",
      "Epoch: 13  Train Loss: 3.7213  Valid Loss: 3.9236\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 3.2475 \n",
      "--------------------------------------------------\n",
      "Epoch: 14  Train Loss: 3.6363  Valid Loss: 3.2475\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 3.2475 at epoch 14/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 385.4403266906738 s\n",
      "Valid Steps: 420/420  Loss: 3.0990 \n",
      "--------------------------------------------------\n",
      "Epoch: 15  Train Loss: 3.4455  Valid Loss: 3.0990\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 3.0990 at epoch 15/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 368.47794795036316 s\n",
      "Valid Steps: 420/420  Loss: 3.1746 \n",
      "--------------------------------------------------\n",
      "Epoch: 16  Train Loss: 3.3082  Valid Loss: 3.1746\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 3.1488 \n",
      "--------------------------------------------------\n",
      "Epoch: 17  Train Loss: 3.1571  Valid Loss: 3.1488\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 3.1070 \n",
      "--------------------------------------------------\n",
      "Epoch: 18  Train Loss: 2.9623  Valid Loss: 3.1070\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 2.7412 \n",
      "--------------------------------------------------\n",
      "Epoch: 19  Train Loss: 2.8511  Valid Loss: 2.7412\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 2.7412 at epoch 19/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 426.0801706314087 s\n",
      "Valid Steps: 420/420  Loss: 2.4713 \n",
      "--------------------------------------------------\n",
      "Epoch: 20  Train Loss: 2.7238  Valid Loss: 2.4713\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 2.4713 at epoch 20/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 411.70828199386597 s\n",
      "Valid Steps: 420/420  Loss: 2.7094 \n",
      "--------------------------------------------------\n",
      "Epoch: 21  Train Loss: 2.5876  Valid Loss: 2.7094\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 2.3478 \n",
      "--------------------------------------------------\n",
      "Epoch: 22  Train Loss: 2.5751  Valid Loss: 2.3478\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 2.3478 at epoch 22/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 372.3716504573822 s\n",
      "Valid Steps: 420/420  Loss: 2.2563 \n",
      "--------------------------------------------------\n",
      "Epoch: 23  Train Loss: 2.4800  Valid Loss: 2.2563\n",
      "--------------------------------------------------\n",
      "\n",
      "Minimum Validation Loss of 2.2563 at epoch 23/100\n",
      "Model Saved\n",
      "\n",
      "Total Elapsed Time : 372.4273421764374 s\n",
      "Valid Steps: 420/420  Loss: 2.6225 \n",
      "--------------------------------------------------\n",
      "Epoch: 24  Train Loss: 2.4087  Valid Loss: 2.6225\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 2.2809 \n",
      "--------------------------------------------------\n",
      "Epoch: 25  Train Loss: 2.3831  Valid Loss: 2.2809\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 2.3030 \n",
      "--------------------------------------------------\n",
      "Epoch: 26  Train Loss: 2.2846  Valid Loss: 2.3030\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 2.2579 \n",
      "--------------------------------------------------\n",
      "Epoch: 27  Train Loss: 2.2526  Valid Loss: 2.2579\n",
      "--------------------------------------------------\n",
      "Valid Steps: 420/420  Loss: 2.4436 \n",
      "--------------------------------------------------\n",
      "Epoch: 28  Train Loss: 2.1661  Valid Loss: 2.4436\n",
      "--------------------------------------------------\n",
      "Train Steps: 30/473  Loss: 2.0969 "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "network = Network()\n",
    "network.cuda()\n",
    "#network.load_state_dict(torch.load('./Vacancy_9.pth'))\n",
    "#network.eval()\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.0001)\n",
    "\n",
    "loss_min = np.inf\n",
    "num_epochs = 100\n",
    "\n",
    "start_time = time.time()\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    epoch_time = time.time()\n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    network.train()\n",
    "    for step in range(1,len(train_loader)+1):\n",
    "    \n",
    "        images, landmarks = next(iter(train_loader))\n",
    "        \n",
    "        images = images.cuda()\n",
    "        landmarks = landmarks.view(landmarks.size(0),-1).cuda()\n",
    "        \n",
    "        predictions = network(images)\n",
    "        \n",
    "        # clear all the gradients before calculating them\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # find the loss for the current step\n",
    "        loss_train_step = criterion(predictions, landmarks)\n",
    "        \n",
    "        # calculate the gradients\n",
    "        loss_train_step.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss_train_step.item()\n",
    "        running_loss = loss_train/step\n",
    "        \n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "        \n",
    "    network.eval() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step in range(1,len(valid_loader)+1):\n",
    "            \n",
    "            images, landmarks = next(iter(valid_loader))\n",
    "        \n",
    "            images = images.cuda()\n",
    "            landmarks = landmarks.view(landmarks.size(0),-1).cuda()\n",
    "        \n",
    "            predictions = network(images)\n",
    "\n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, landmarks)\n",
    "\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, 'valid')\n",
    "    \n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "    train_losses.insert(epoch, loss_train)\n",
    "    valid_losses.insert(epoch, loss_valid)\n",
    "    print('\\n--------------------------------------------------')\n",
    "    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    print('--------------------------------------------------')\n",
    "    \n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(network.state_dict(), 'Vacancy_16.pth') # 1 : dataset : 300W & openCVDNN, network : resnet34(3,1,2), preprocess : 50% of openCVDNN\n",
    "                                                          # 2 : dataset 3219->3816(multi-face images not ommitted)\n",
    "                                                          # 3 : dataset split (open only)\n",
    "                                                          # 7 : oversampling & downsampling (0.5 : 19)\n",
    "                                                          # 8 : oversampling & downsampling (0.25 : 9)\n",
    "                                                          # 9 : oversampling (1:39)\n",
    "                                                          # 10 : oversampling (1:39) WITH 200 EPOCHS\n",
    "                                                          # 11 : oversampling (1:39) with smaller kernel\n",
    "                                                          # 12 : data : 300w, flips added, pretraiend on (10+ : overfitting)\n",
    "                                                          # 13 : data : 300w, flips added, pretraiend off\n",
    "                                                          # 15 : network : coarsed\n",
    "                                                          # 16 : w/o flips. 300w added only from 11\n",
    "\n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "        print('Model Saved\\n')\n",
    "        print(\"Total Elapsed Time : {} s\".format(time.time()-epoch_time))\n",
    "     \n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))\n",
    "\n",
    "import csv\n",
    "with open('Vacancy_16_epochs.csv', 'w', newline='') as f: \n",
    "    writer = csv.writer(f) \n",
    "    writer.writerow(train_losses) \n",
    "    writer.writerow(valid_losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed95613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
